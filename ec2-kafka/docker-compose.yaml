# =============================================================================
# Kafka Stack for EC2 - Zookeeper + Kafka Broker + Kafka Connect
# =============================================================================
# IMPORTANT: Replace YOUR_EC2_PUBLIC_IP with your actual EC2 public IP address!
# Example: EXTERNAL://54.123.45.67:9092
# =============================================================================

version: '3.8'

services:
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    restart: unless-stopped
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
      # Reduce memory for t3.micro
      KAFKA_HEAP_OPTS: "-Xmx256m -Xms128m"

  broker:
    image: confluentinc/cp-kafka:7.5.0
    container_name: broker
    restart: unless-stopped
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181'
      
      # =======================================================================
      # LISTENER CONFIGURATION - This is the tricky part!
      # INTERNAL: Used by containers inside EC2 (Connect talks to Broker)
      # EXTERNAL: Used by your laptop (PySpark connects from outside)
      # =======================================================================
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://broker:29092,EXTERNAL://13.60.200.41:9092
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL
      
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: 'true'
      KAFKA_LOG_RETENTION_HOURS: 168  # Keep data for 7 days
      
      # Reduce memory for t3.micro
      KAFKA_HEAP_OPTS: "-Xmx384m -Xms256m"

  connect:
    image: confluentinc/cp-kafka-connect:7.5.0
    container_name: connect
    restart: unless-stopped
    depends_on:
      - broker
    ports:
      - "8083:8083"
    environment:
      CONNECT_BOOTSTRAP_SERVERS: 'broker:29092'
      CONNECT_REST_ADVERTISED_HOST_NAME: connect
      CONNECT_GROUP_ID: s3-source-group
      
      # Internal Kafka topics for Connect
      CONNECT_CONFIG_STORAGE_TOPIC: docker-connect-configs
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_OFFSET_STORAGE_TOPIC: docker-connect-offsets
      CONNECT_OFFSET_STORAGE_REPLICATION_FACTOR: 1
      CONNECT_STATUS_STORAGE_TOPIC: docker-connect-status
      CONNECT_STATUS_STORAGE_REPLICATION_FACTOR: 1
      
      # Converters - Keep as strings since we handle JSON in Spark
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.storage.StringConverter
      
      CONNECT_PLUGIN_PATH: "/usr/share/java,/usr/share/confluent-hub-components"
      
      # =======================================================================
      # AWS CREDENTIALS - Replace with your actual keys!
      # =======================================================================
      AWS_ACCESS_KEY_ID: "${AWS_ACCESS_KEY_ID}"
      AWS_SECRET_ACCESS_KEY: "${AWS_SECRET_ACCESS_KEY}"
      
      # Reduce memory for t3.micro
      KAFKA_HEAP_OPTS: "-Xmx384m -Xms256m"
    
    command:
      - bash
      - -c
      - |
        echo "Installing S3 Source Connector..."
        confluent-hub install --no-prompt confluentinc/kafka-connect-s3-source:2.6.0
        echo "Starting Kafka Connect..."
        /etc/confluent/docker/run

